<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="Chemputer Blog"><meta property="og:type" content="article"><meta property="og:image" content="img/ai.jpg"><meta property="twitter:image" content="img/ai.jpg"><meta name=title content="《Machine Learning》学习笔记"><meta property="og:title" content="《Machine Learning》学习笔记"><meta property="twitter:title" content="《Machine Learning》学习笔记"><meta name=description content="By Andrew Ng “Donot worry about it”"><meta property="og:description" content="By Andrew Ng “Donot worry about it”"><meta property="twitter:description" content="By Andrew Ng “Donot worry about it”"><meta property="twitter:card" content="summary"><meta name=keyword content="汪洋龙, wangyanglong, Wangyanglong, 汪洋龙的博客, Wangyanglong Blog, 博客, 个人网站, 互联网, Web, Javascript, React"><link rel="shortcut icon" href=/img/favicon.ico><title>《Machine Learning》学习笔记 | 汪洋龙的博客 | Wangyanglong Blog</title><link rel=canonical href=/post/ml/andrewng/ml/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.css><link href=https://cdn.jsdelivr.net/gh/FortAwesome/Font-Awesome@5.15.1/css/all.css rel=stylesheet type=text/css><script src=/js/jquery.min.js></script>
<script src=/js/bootstrap.min.js></script>
<script src=/js/hux-blog.min.js></script>
<script src=/js/lazysizes.min.js></script>
<script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-5E5ZJN0DQ4"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5E5ZJN0DQ4",{anonymize_ip:!1})}</script><style>code.has-jax{-webkit-font-smoothing:antialiased;background:inherit!important;border:none!important;font-size:100%}</style><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button>
<a class=navbar-brand href=/>Chemputer Blog</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/categories/es6/>es6</a></li><li><a href=/categories/react/>react</a></li><li><a href=/categories/tech/>tech</a></li><li><a href=/archive//>ARCHIVE</a></li><li><a href=/notes//>NOTES</a></li><li><a href=/photos//>PHOTOS</a></li><li><a href=/about//>ABOUT</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(/img/ai.jpg)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/ai title=ai>ai</a></div><h1>《Machine Learning》学习笔记</h1><h2 class=subheading></h2><span class=meta>Posted by
汪洋龙
on
Thursday, March 23, 2023</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><h1 id=机器学习>机器学习</h1><blockquote><p><a href=https://www.bilibili.com/video/BV1Pa411X76s>BiliBili《2022 吴恩达机器学习》</a></p></blockquote><h2 id=1-机器学习算法>1. 机器学习算法</h2><h3 id=11-监督学习-supervised-learning>1.1 监督学习 (Supervised Learning)</h3><p><strong>Learns from data <code>labeled</code> with the &ldquo;<code>right answers</code>&rdquo;</strong></p><p>将输入（input）x 映射到输出（output）y，从给定的正确答案中学习。</p><ul><li>回归算法 (regression)：预测房屋尺寸和房价的关系</li><li>分类算法 (classification)：判断良性/恶性肿瘤</li></ul><h3 id=12-无监督学习-unsupervised-learning>1.2 无监督学习 (Unsupervised Learning)</h3><p><strong>Find something interesting in <code>unlabeled</code> data</strong></p><ul><li>聚类算法 (clustering algorithm)： Google news、DNA 微阵列数据</li><li>异常检测 (Anomaly detection)：找到异常数据</li><li>降维 (Dimensionality reduction)：压缩数据得到更小的数据集</li></ul><h3 id=13-jupyter-notebook>1.3 Jupyter Notebook</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># pip 安装</span>
</span></span><span style=display:flex><span>pip install jupyterlab
</span></span><span style=display:flex><span><span style=color:#6272a4># 启动</span>
</span></span><span style=display:flex><span>jupyter-lab
</span></span></code></pre></div><h2 id=2-线性回归linear-regression>2. 线性回归（linear regression）</h2><h3 id=21-一些定义>2.1 一些定义</h3><p>在机器学习中</p><ul><li><code>$\hat{y}$</code> 表示对 <code>$y$</code> 的预测</li><li><code>$f$</code> 函数称为模型</li><li><code>$x$</code> 成为输入或者输入特征</li></ul><h3 id=22-如何表示-f>2.2 如何表示 $f$</h3><p>用线性函数 <code>$f(x) = wx +b$</code> 表示，这样简单易用。并且以此作为基础，可以帮助获得更复杂的非线性模型。</p><h3 id=23-代价函数cost-function>2.3 代价函数（Cost Function）</h3><p>有了函数之后，现在的问题变成了：找到合适的 <code>$w$</code> 和 <code>$b$</code>，以便于更好的贴合训练集的真实数据。</p><p>我们创建一个<code>代价函数</code>（Cost Function）来衡量一条线与训练数据的拟合程度</p><p>构建出一个 <code>$J(w,b)=\frac{1}{2m} \sum_{i=1}^{m} (\hat{y}-y_{n})^{2} $</code>的公式（求平均有 <code>2</code> 是出于好计算）</p><p>因为有平方，所以也被称为<code>平方误差代价函数</code> (Squared error cost function)。</p><p>在机器学习中，不同的人会用不同的代价函数，但迄今为止，平方误差代价函数通常用于线性回归，并且会得到很好的结果。</p><h3 id=24-理解代价函数>2.4 理解代价函数</h3><p>我们最终的目的，是找到 <code>$J$</code> 最小值对应的参数 <code>$w$</code> 和 <code>$b$</code></p><h3 id=25-可视化代价函数>2.5 可视化代价函数</h3><p>拥有<code>$w$</code> 和 <code>$b$</code>两个参数的 <code>$J$</code> 函数是一个三维的碗状图</p><p><img src=/post/ml/AndrewNg/images/3d-plot.jpg alt=可视化代价函数></p><h2 id=3-梯度下降gradient-descent>3. 梯度下降（Gradient Descent）</h2><p>梯度下降是一种算法，来尝试最小化任何函数</p><h3 id=31-实现梯度下降>3.1 实现梯度下降</h3><p><code>$\alpha $</code> 称为学习率（Learning rate），通常介于 0 和 1 之间。</p><p><code>$w$</code> 和 <code>$b$</code> 的数据需要<code>同时更新</code></p><p><code>$tmp\_w=w-\alpha \frac{\partial J(w,b )}{\partial w} $</code></p><p><code>$tmp\_b=b-\alpha \frac{\partial J(w,b )}{\partial b}$</code></p><p><code>$w=tmp\_w$</code></p><p><code>$b=tmp\_b$</code></p><h3 id=32-理解梯度下降>3.2 理解梯度下降</h3><p>斜率为正数时，<code>$w$</code> 减小向左移；斜率为负数时，<code>$w$</code> 增加向右移。</p><p><img src=/post/ml/AndrewNg/images/gd.jpg alt=理解梯度下降></p><h3 id=33-学习率>3.3 学习率</h3><p>学习率的选择至关重要。如果过小，会很慢；如果过大，可能会越过最小值，导致函数不能收敛甚至发散。</p><h3 id=34-用于线性回归的梯度下降>3.4 用于线性回归的梯度下降</h3><p>将公式带入之后会得到：</p><p><code>$w = w - \alpha \frac{1}{m} \sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})x^{(i)}$</code></p><p><code>$b = b - \alpha \frac{1}{m} \sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})$</code></p><h3 id=35-运行梯度下降>3.5 运行梯度下降</h3><p>更准确的说，这种梯度下降也被称为<code>批量梯度下降</code>（Batch gradient descent）。<code>Batch</code> 表示在进行梯度下降的每一步，都会用到 <code>整个训练集</code>。</p><h2 id=4-多维特征>4. 多维特征</h2><p>对于上面 <code>$f_{w,b}(x) = wx + b$</code> 这个单个特征模型来说，现实会有更多的维度。</p><p>具有 <code>$n$</code> 个特征的模型会是如下的定义:</p><p><code>$f_{w,b}(x) = w_{1}x_{1}+w_{2}x_{2}+\dots +w_{n}x_{n}+b$</code>。</p><p>我们不妨将它更简洁的表示：</p><p><code>$\vec{w}=\begin{bmatrix}w_{1} & w_{2} & w_{3} & \dots & w_{n}\end{bmatrix}$</code></p><p><code>$\vec{x}=\begin{bmatrix}x_{1} & x_{2} & x_{3} & \dots & x_{n}\end{bmatrix}$</code></p><p><code>$f_{\vec{w},b}(\vec{x})= \vec{w}\cdot \vec{x} + b$</code></p><p>这种具有多个输入特征的线性回归称为<code>多元线性回归</code>。为了方便实现，有一个很巧妙的技巧：<code>矢量化</code>。</p><h3 id=41-矢量化>4.1 矢量化</h3><p>如果不用矢量，用程序实现 <code>$f_{\vec{w},b}(\vec{x})= \sum_{i=1}^{n}w_{j}x_{j}+b$</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>f <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>for</span> j <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#bd93f9>0</span>, n):
</span></span><span style=display:flex><span>    f <span style=color:#ff79c6>=</span> f <span style=color:#ff79c6>+</span> w[j] <span style=color:#ff79c6>*</span> x[j]
</span></span><span style=display:flex><span>f <span style=color:#ff79c6>=</span> f <span style=color:#ff79c6>+</span> b
</span></span></code></pre></div><p>用矢量，即实现 <code>$f_{\vec{w},b}(\vec{x})= \vec{w}\cdot \vec{x} + b$</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#ff79c6>import</span> numpy <span style=color:#ff79c6>as</span> np
</span></span><span style=display:flex><span>f <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>dot(w,x) <span style=color:#ff79c6>+</span> b
</span></span></code></pre></div><p>优点： 1. 代码<code>编写简单</code> 2. 运行<code>效率高</code>（Numpy 的 dot 函数能使用并行硬件的能力）</p><h3 id=42-用于多元线性回归的梯度下降>4.2 用于多元线性回归的梯度下降</h3><p><code>$w_{n}=w_{n}-\alpha \frac{1}{m} \sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x} ^{(i)})-y^{(i)})x_{n} ^{(i)}$</code></p><p><code>$b=b-\alpha \frac{1}{m} \sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x} ^{(i)})-y^{(i)})$</code></p><h3 id=43-特征缩放>4.3 特征缩放</h3><p>目的提高效率。</p><p><img src=/post/ml/AndrewNg/images/feature-scale.jpg alt=特征缩放></p><p>用图更直观的表示：当存在不同特征值时，取值范围不同，会导致梯度下降缓慢。但重新缩放不同特征，让它们处在可比较的范围内，速度和梯度的更新会显著提升。</p><h3 id=44-如何实现特征缩放>4.4 如何实现特征缩放</h3><ul><li>均值归一化（Mean normalization）</li><li>Z-score 标准化（Z-score normalization）</li></ul><h3 id=45-判断梯度下降是否收敛>4.5 判断梯度下降是否收敛</h3><p>梯度下降时，通过观察迭代次数 <code>Iterations</code> 和代价函数 <code>$J(\vec{w},b)$</code> 所构建的 <code>学习曲线</code>。</p><h3 id=46-选择合适的学习率>4.6 选择合适的学习率</h3><p>过小的学习率会让迭代成本增加；过大则可能会导致发散。<code>Andrew Ng</code> 的习惯是用 <code>3x</code> 的策略去寻找并绘制合适的学习曲线。</p><p><img src=/post/ml/AndrewNg/images/choose-alpha.jpg alt=选择合适的学习率></p><h3 id=47-特征工程feature-engineering>4.7 特征工程（Feature engineering）</h3><p>对于许多实际应用中，选择或输入正确的特征是使算法运行良好的关键步骤。</p><p><code>Feature engineering: Using intuition to design new features, by transforming or combining original features</code></p><p>有时候通过定义新特性，可以得到更好的模型。</p><h3 id=48-多项式回归>4.8 多项式回归</h3><p>相比直线而言，曲线更符合大多数情况。<a href=https://scikit-learn.org/stable/>scikit-learn</a> 是一个开源的机器学习库，可以把它用于你训练的模型中。</p><h2 id=5-逻辑回归>5. 逻辑回归</h2><p>对于分类问题，线性回归最佳的拟合线不能解决问题。逻辑回归则用于解决输出标签 <code>$y$</code> 是 <code>0</code> 或 <code>1</code> 的二元分类问题。</p><h3 id=51-模型定义>5.1 模型定义</h3><p><img src=/post/ml/AndrewNg/images/lr.jpg alt=逻辑回归></p><h3 id=52-决策边界>5.2 决策边界</h3><ul><li>线性决策边界</li><li>非线形决策边界</li></ul><h3 id=53-逻辑回归中的代价函数>5.3 逻辑回归中的代价函数</h3><p><img src=/post/ml/AndrewNg/images/lr-costfn1.jpg alt=逻辑回归中的代价函数></p><p>如果用线形回归的代价函数，套用到逻辑回归的代价函数中，会得到一个非凸函数，这样不适用于梯度下降。</p><p><img src=/post/ml/AndrewNg/images/lr-costfn2.jpg alt=逻辑回归中的代价函数></p><h3 id=54-简化逻辑回归代价函数>5.4 简化逻辑回归代价函数</h3><p><img src=/post/ml/AndrewNg/images/lr-costfn3.jpg alt=简化逻辑回归代价函数></p><h3 id=55-实现梯度下降>5.5 实现梯度下降</h3><p><img src=/post/ml/AndrewNg/images/lr-gd1.jpg alt=实现梯度下降></p><p><img src=/post/ml/AndrewNg/images/lr-gd2.jpg alt=实现梯度下降></p><h2 id=6-过拟合问题>6. 过拟合问题</h2><p>线形回归过拟合</p><p><img src=/post/ml/AndrewNg/images/overfit1.jpg alt=过拟合问题></p><p>分类过拟合</p><p><img src=/post/ml/AndrewNg/images/overfit2.jpg alt=过拟合问题></p><p><strong>过于贴合训练集的数据，导致不能很好的泛化。</strong></p><h3 id=61-解决过拟合问题>6.1 解决过拟合问题</h3><ul><li>收集更多的训练集（但并不总是会有很多的数据可供收集）</li><li>减少特征选项</li><li>正则化（温和减少特征，鼓励学习算法缩小参数）</li></ul><h3 id=62-正则化>6.2 正则化</h3><h3 id=63-用于线性回归的正则化>6.3 用于线性回归的正则化</h3><h3 id=64-用于逻辑回归的正则化>6.4 用于逻辑回归的正则化</h3><hr><ul class=pager><li class=previous><a href=/post/threejs/3dmap/ data-toggle=tooltip data-placement=top title=ThreeJS实现3D中国地图>&larr;
Previous Post</a></li><li class=next><a href=/post/blog/extend-latex/ data-toggle=tooltip data-placement=top title="Hugo 主题添加 Latex 支持">Next
Post &rarr;</a></li></ul></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/blender title=blender>blender</a>
<a href=/tags/blog title=blog>blog</a>
<a href=/tags/infrastructure title=infrastructure>infrastructure</a>
<a href=/tags/learningopengl title=learningopengl>learningopengl</a>
<a href=/tags/qiankun title=qiankun>qiankun</a>
<a href=/tags/raspberrypi title=raspberrypi>raspberrypi</a>
<a href=/tags/react title=react>react</a>
<a href=/tags/ts title=ts>ts</a>
<a href=/tags/%E5%88%86%E4%BA%AB%E4%BC%9A title=分享会>分享会</a>
<a href=/tags/%E5%A5%87%E7%BB%A9%E5%88%9B%E5%9D%9B title=奇绩创坛>奇绩创坛</a>
<a href=/tags/%E8%AF%91%E6%96%87 title=译文>译文</a></div></section></div></div></div></article><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a target=_blank href=https://github.com/chemistwang><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a href rel=alternate type=application/rss+xml title="Chemputer Blog"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; Chemputer Blog 2023<br><a href=https://themes.gohugo.io/hugo-theme-cleanwhite>CleanWhite Hugo Theme</a> by <a href=https://zhaohuabing.com>Huabing</a> |
<iframe style=margin-left:2px;margin-bottom:-5px frameborder=0 scrolling=0 width=100px height=20px src="https://ghbtns.com/github-btn.html?user=zhaohuabing&repo=hugo-theme-cleanwhite&type=star&count=true"></iframe></p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,a=$(_containerSelector),r=a.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),r.each(function(){t=$(this).prop("tagName").toLowerCase(),o="#"+$(this).prop("id"),n=$(this).text(),i=$('<a href="'+o+'" rel="nofollow">'+n+"</a>"),s=$('<li class="'+t+'_nav"></li>').append(i),$(e).append(s)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script></body></html>